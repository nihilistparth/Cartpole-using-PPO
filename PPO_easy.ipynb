{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b87dda9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2068c8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gym\n",
    "import time\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i+self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states),\\\n",
    "                np.array(self.actions),\\\n",
    "                np.array(self.probs),\\\n",
    "                np.array(self.vals),\\\n",
    "                np.array(self.rewards),\\\n",
    "                np.array(self.dones),\\\n",
    "                batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, alpha,\n",
    "            fc1_dims=256, fc2_dims=256, chkpt_dir= \"CHKPT\"):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, 'actor_torch_ppo')\n",
    "#         self.checkpoint_file = chkpt_dir\n",
    "        self.actor = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, n_actions),\n",
    "                nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        \n",
    "        return dist\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, alpha, fc1_dims=256, fc2_dims=256,\n",
    "            chkpt_dir=\"CHKPT\"):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir,'critic_torch_ppo')\n",
    "\n",
    "        \n",
    "        self.critic = nn.Sequential(\n",
    "                nn.Linear(*input_dims, fc1_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc1_dims, fc2_dims),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(fc2_dims, 1)\n",
    "        )\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.99, alpha=0.0003, gae_lambda=0.95,\n",
    "            policy_clip=0.2, batch_size=64, n_epochs=10):\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "\n",
    "        self.actor = ActorNetwork(n_actions, input_dims, alpha)\n",
    "        self.critic = CriticNetwork(input_dims, alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "       \n",
    "    def remember(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        print('... saving models ...')\n",
    "        self.actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        print('... loading models ...')\n",
    "        self.actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "\n",
    "        dist = self.actor(state)\n",
    "        value = self.critic(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        probs = T.squeeze(dist.log_prob(action)).item()\n",
    "        action = T.squeeze(action).item()\n",
    "        value = T.squeeze(value).item()\n",
    "\n",
    "        return action, probs, value\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr,\\\n",
    "            reward_arr, dones_arr, batches = \\\n",
    "                    self.memory.generate_batches()\n",
    "\n",
    "            values = vals_arr\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr)-1):\n",
    "                    a_t += discount*(reward_arr[k] + self.gamma*values[k+1]*\\\n",
    "                            (1-int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma*self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "            advantage = T.tensor(advantage).to(self.actor.device)\n",
    "\n",
    "            values = T.tensor(values).to(self.actor.device)\n",
    "            for batch in batches:\n",
    "                states = T.tensor(state_arr[batch], dtype=T.float).to(self.actor.device)\n",
    "                old_probs = T.tensor(old_prob_arr[batch]).to(self.actor.device)\n",
    "                actions = T.tensor(action_arr[batch]).to(self.actor.device)\n",
    "\n",
    "                dist = self.actor(states)\n",
    "                critic_value = self.critic(states)\n",
    "\n",
    "                critic_value = T.squeeze(critic_value)\n",
    "\n",
    "                new_probs = dist.log_prob(actions)\n",
    "                prob_ratio = new_probs.exp() / old_probs.exp()\n",
    "                #prob_ratio = (new_probs - old_probs).exp()\n",
    "                weighted_probs = advantage[batch] * prob_ratio\n",
    "                weighted_clipped_probs = T.clamp(prob_ratio, 1-self.policy_clip,\n",
    "                        1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss = -T.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns-critic_value)**2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                total_loss = actor_loss + 0.5*critic_loss\n",
    "                self.actor.optimizer.zero_grad()\n",
    "                self.critic.optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                self.actor.optimizer.step()\n",
    "                self.critic.optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41405cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8377b0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving models ...\n",
      "episode 0 score 29.0 avg score 29.0 time_steps 29 learning_steps 1\n",
      "episode 1 score 17.0 avg score 23.0 time_steps 46 learning_steps 2\n",
      "episode 2 score 18.0 avg score 21.3 time_steps 64 learning_steps 3\n",
      "episode 3 score 35.0 avg score 24.8 time_steps 99 learning_steps 4\n",
      "episode 4 score 12.0 avg score 22.2 time_steps 111 learning_steps 5\n",
      "episode 5 score 23.0 avg score 22.3 time_steps 134 learning_steps 6\n",
      "episode 6 score 27.0 avg score 23.0 time_steps 161 learning_steps 8\n",
      "episode 7 score 28.0 avg score 23.6 time_steps 189 learning_steps 9\n",
      "episode 8 score 10.0 avg score 22.1 time_steps 199 learning_steps 9\n",
      "episode 9 score 27.0 avg score 22.6 time_steps 226 learning_steps 11\n",
      "episode 10 score 23.0 avg score 22.6 time_steps 249 learning_steps 12\n",
      "episode 11 score 35.0 avg score 23.7 time_steps 284 learning_steps 14\n",
      "episode 12 score 35.0 avg score 24.5 time_steps 319 learning_steps 15\n",
      "episode 13 score 49.0 avg score 26.3 time_steps 368 learning_steps 18\n",
      "episode 14 score 31.0 avg score 26.6 time_steps 399 learning_steps 19\n",
      "episode 15 score 22.0 avg score 26.3 time_steps 421 learning_steps 21\n",
      "episode 16 score 32.0 avg score 26.6 time_steps 453 learning_steps 22\n",
      "episode 17 score 14.0 avg score 25.9 time_steps 467 learning_steps 23\n",
      "episode 18 score 13.0 avg score 25.3 time_steps 480 learning_steps 24\n",
      "episode 19 score 10.0 avg score 24.5 time_steps 490 learning_steps 24\n",
      "episode 20 score 11.0 avg score 23.9 time_steps 501 learning_steps 25\n",
      "episode 21 score 12.0 avg score 23.3 time_steps 513 learning_steps 25\n",
      "episode 22 score 11.0 avg score 22.8 time_steps 524 learning_steps 26\n",
      "episode 23 score 13.0 avg score 22.4 time_steps 537 learning_steps 26\n",
      "episode 24 score 11.0 avg score 21.9 time_steps 548 learning_steps 27\n",
      "episode 25 score 27.0 avg score 22.1 time_steps 575 learning_steps 28\n",
      "episode 26 score 18.0 avg score 22.0 time_steps 593 learning_steps 29\n",
      "episode 27 score 17.0 avg score 21.8 time_steps 610 learning_steps 30\n",
      "episode 28 score 25.0 avg score 21.9 time_steps 635 learning_steps 31\n",
      "episode 29 score 28.0 avg score 22.1 time_steps 663 learning_steps 33\n",
      "episode 30 score 22.0 avg score 22.1 time_steps 685 learning_steps 34\n",
      "episode 31 score 21.0 avg score 22.1 time_steps 706 learning_steps 35\n",
      "episode 32 score 21.0 avg score 22.0 time_steps 727 learning_steps 36\n",
      "episode 33 score 36.0 avg score 22.4 time_steps 763 learning_steps 38\n",
      "episode 34 score 29.0 avg score 22.6 time_steps 792 learning_steps 39\n",
      "episode 35 score 35.0 avg score 23.0 time_steps 827 learning_steps 41\n",
      "episode 36 score 27.0 avg score 23.1 time_steps 854 learning_steps 42\n",
      "episode 37 score 35.0 avg score 23.4 time_steps 889 learning_steps 44\n",
      "episode 38 score 27.0 avg score 23.5 time_steps 916 learning_steps 45\n",
      "episode 39 score 54.0 avg score 24.2 time_steps 970 learning_steps 48\n",
      "episode 40 score 53.0 avg score 25.0 time_steps 1023 learning_steps 51\n",
      "episode 41 score 33.0 avg score 25.1 time_steps 1056 learning_steps 52\n",
      "episode 42 score 28.0 avg score 25.2 time_steps 1084 learning_steps 54\n",
      "episode 43 score 22.0 avg score 25.1 time_steps 1106 learning_steps 55\n",
      "episode 44 score 20.0 avg score 25.0 time_steps 1126 learning_steps 56\n",
      "episode 45 score 42.0 avg score 25.4 time_steps 1168 learning_steps 58\n",
      "episode 46 score 60.0 avg score 26.1 time_steps 1228 learning_steps 61\n",
      "episode 47 score 58.0 avg score 26.8 time_steps 1286 learning_steps 64\n",
      "episode 48 score 53.0 avg score 27.3 time_steps 1339 learning_steps 66\n",
      "... saving models ...\n",
      "episode 49 score 128.0 avg score 29.3 time_steps 1467 learning_steps 73\n",
      "... saving models ...\n",
      "episode 50 score 41.0 avg score 29.6 time_steps 1508 learning_steps 75\n",
      "... saving models ...\n",
      "episode 51 score 52.0 avg score 30.0 time_steps 1560 learning_steps 78\n",
      "... saving models ...\n",
      "episode 52 score 50.0 avg score 30.4 time_steps 1610 learning_steps 80\n",
      "... saving models ...\n",
      "episode 53 score 90.0 avg score 31.5 time_steps 1700 learning_steps 85\n",
      "... saving models ...\n",
      "episode 54 score 39.0 avg score 31.6 time_steps 1739 learning_steps 86\n",
      "... saving models ...\n",
      "episode 55 score 69.0 avg score 32.3 time_steps 1808 learning_steps 90\n",
      "... saving models ...\n",
      "episode 56 score 124.0 avg score 33.9 time_steps 1932 learning_steps 96\n",
      "... saving models ...\n",
      "episode 57 score 160.0 avg score 36.1 time_steps 2092 learning_steps 104\n",
      "... saving models ...\n",
      "episode 58 score 121.0 avg score 37.5 time_steps 2213 learning_steps 110\n",
      "... saving models ...\n",
      "episode 59 score 69.0 avg score 38.0 time_steps 2282 learning_steps 114\n",
      "... saving models ...\n",
      "episode 60 score 74.0 avg score 38.6 time_steps 2356 learning_steps 117\n",
      "... saving models ...\n",
      "episode 61 score 59.0 avg score 39.0 time_steps 2415 learning_steps 120\n",
      "... saving models ...\n",
      "episode 62 score 108.0 avg score 40.0 time_steps 2523 learning_steps 126\n",
      "... saving models ...\n",
      "episode 63 score 182.0 avg score 42.3 time_steps 2705 learning_steps 135\n",
      "... saving models ...\n",
      "episode 64 score 152.0 avg score 44.0 time_steps 2857 learning_steps 142\n",
      "... saving models ...\n",
      "episode 65 score 200.0 avg score 46.3 time_steps 3057 learning_steps 152\n",
      "... saving models ...\n",
      "episode 66 score 138.0 avg score 47.7 time_steps 3195 learning_steps 159\n",
      "... saving models ...\n",
      "episode 67 score 177.0 avg score 49.6 time_steps 3372 learning_steps 168\n",
      "... saving models ...\n",
      "episode 68 score 155.0 avg score 51.1 time_steps 3527 learning_steps 176\n",
      "... saving models ...\n",
      "episode 69 score 126.0 avg score 52.2 time_steps 3653 learning_steps 182\n",
      "... saving models ...\n",
      "episode 70 score 200.0 avg score 54.3 time_steps 3853 learning_steps 192\n",
      "... saving models ...\n",
      "episode 71 score 200.0 avg score 56.3 time_steps 4053 learning_steps 202\n",
      "... saving models ...\n",
      "episode 72 score 200.0 avg score 58.3 time_steps 4253 learning_steps 212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "    env = gym.make('CartPole-v0')\n",
    "    N = 20\n",
    "    batch_size = 5\n",
    "    n_epochs = 4\n",
    "    alpha = 0.0003\n",
    "    agent = Agent(n_actions=env.action_space.n, batch_size=batch_size, \n",
    "                    alpha=alpha, n_epochs=n_epochs, \n",
    "                    input_dims=env.observation_space.shape)\n",
    "    n_games = 300\n",
    "\n",
    "    figure_file = 'plots/cartpole.png'\n",
    "\n",
    "    best_score = env.reward_range[0]\n",
    "    score_history = []\n",
    "\n",
    "    learn_iters = 0\n",
    "    avg_score = 0\n",
    "    n_steps = 0\n",
    "\n",
    "    for i in range(n_games):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        while not done:\n",
    "            action, prob, val = agent.choose_action(observation)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            n_steps += 1\n",
    "            score += reward\n",
    "            agent.remember(observation, action, prob, val, reward, done)\n",
    "            if n_steps % N == 0:\n",
    "                agent.learn()\n",
    "                learn_iters += 1\n",
    "            observation = observation_\n",
    "        score_history.append(score)\n",
    "        avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            agent.save_models()\n",
    "\n",
    "        print('episode', i, 'score %.1f' % score, 'avg score %.1f' % avg_score,\n",
    "                'time_steps', n_steps, 'learning_steps', learn_iters)\n",
    "    x = [i+1 for i in range(len(score_history))]\n",
    "    plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb98cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da90797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2efc4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
